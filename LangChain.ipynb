{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Coding AI Agents in Python\n",
    "\n",
    "#### By Pedro Izquierdo Lehmann\n",
    "\n",
    "Welcome to this hands-on introduction to **LangChain**! This notebook will guide you through building intelligent AI agents that can use tools, remember conversations, and make decisions autonomously.\n",
    "\n",
    "**What is LangChain?**\n",
    "LangChain is a framework for developing applications powered by language models. With it you can build explicit **chains**, which is an abstraction of an algorithm involving LLMs calls. Also, LangChain promotes implicit chains: instead of just asking an LLM questions, you can give it **tools** to use, and it will intelligently decide when and how to use them to answer your questions, instead of writing complex routing logic. \n",
    "\n",
    "LangChain works with the abstraction of the objects involved in the agentic system, such as\n",
    "\n",
    "- **Chains**: Abstraction of an algorithm involving multiple steps; a reusable workflow.\n",
    "- **Agents**: Abstraction of an LLM model equipped with tools, which can decide which tools/steps to run (an implicit chain).\n",
    "- **Tools**: Wrapped Python functions so the agent can call them.\n",
    "- **Memory/State**: Abstraction of context across conversation.\n",
    "\n",
    "LangChain orders these in **layers** of abstraction, so you can start simple and add power only when you need it. Each layer builds on the previous one. This notebook follows that same progression: we start with tools, then add memory, context, and structured outputs.\n",
    "\n",
    "**Content:**\n",
    "- Creating your first AI agent\n",
    "- Building custom tools for agents to use\n",
    "- Adding memory so agents remember past conversations\n",
    "- Using structured output for consistent responses\n",
    "- Context-aware tools that access user information\n",
    "- Best practices for production-ready agents\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (0. Environment Setup)\n",
    "\n",
    "Before starting, you need to set up a Python virtual environment and install all required dependencies. Follow these steps:\n",
    "\n",
    "#### 1. Create a Virtual Environment\n",
    "\n",
    "Open your terminal and navigate to the **directory containing this notebook**, then run:\n",
    "\n",
    "```bash\n",
    "python3 -m venv lang-chain\n",
    "```\n",
    "\n",
    "This creates a virtual environment in a folder called `lang-chain`.\n",
    "\n",
    "#### 2. Activate the Virtual Environment\n",
    "\n",
    "**On macOS/Linux:**\n",
    "```bash\n",
    "source lang-chain/bin/activate\n",
    "```\n",
    "\n",
    "**On Windows:**\n",
    "```bash\n",
    "lang-chain\\Scripts\\activate\n",
    "```\n",
    "\n",
    "You should see `(lang-chain)` at the beginning of your terminal prompt, indicating the virtual environment is active.\n",
    "\n",
    "#### 3. Install Required Dependencies\n",
    "\n",
    "With the virtual environment activated, install all necessary packages:\n",
    "\n",
    "```bash\n",
    "pip install langchain langgraph langchain-anthropic langchain-openai jupyter ipykernel\n",
    "```\n",
    "\n",
    "This will install:\n",
    "- `langchain` - The core LangChain framework\n",
    "- `langgraph` - For building stateful agent workflows and checkpointers\n",
    "- `langchain-anthropic` - Anthropic (Claude) model provider\n",
    "- `langchain-openai` - OpenAI model provider\n",
    "- `jupyter` - Jupyter notebook environment\n",
    "- `ipykernel` - Jupyter kernel for the virtual environment\n",
    "\n",
    "Register the virtual environment as a Jupyter kernel:\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name=lang-chain --display-name \"Python (lang-chain)\"\n",
    "```\n",
    "\n",
    "This ensures Jupyter can use your virtual environment's Python interpreter.\n",
    "\n",
    "#### 4. Start Jupyter Notebook\n",
    "\n",
    "We recommend two options to run the notebook:\n",
    "\n",
    "**Jupyter Notebook:**\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "This will open Jupyter in your web browser. Navigate to and open this notebook (`LangChain.ipynb`).\n",
    "\n",
    "**Code Editor like VS Code or Cursor:**\n",
    "\n",
    "1. Open the notebook file (`LangChain.ipynb`) in your code editor\n",
    "2. The editor should automatically detect it as a Jupyter notebook\n",
    "3. When prompted to select a kernel, choose **Python (lang-chain)** from the list\n",
    "4. If the kernel doesn't appear, you may need to refresh the kernel list or ensure the virtual environment is properly registered\n",
    "\n",
    "#### 5. Deactivate\n",
    "\n",
    "Don't forget to deactivate the virtual environment when you're done working with the following command:\n",
    "\n",
    "```bash\n",
    "deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m venv lang-chain\n",
    "# source lang-chain/bin/activate\n",
    "# lang-chain\\Scripts\\activate\n",
    "# pip install langchain langgraph langchain-anthropic langchain-openai jupyter ipykernel\n",
    "# python -m ipykernel install --user --name=lang-chain --display-name \"Python (lang-chain)\"\n",
    "# jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_agent' from 'langchain.agents' (/Users/ywo/anaconda3/lib/python3.11/site-packages/langchain/agents/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_agent\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_chat_model\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tool, ToolRuntime\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_agent' from 'langchain.agents' (/Users/ywo/anaconda3/lib/python3.11/site-packages/langchain/agents/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "# Set API key\n",
    "api_key = 'my api'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chains\n",
    "\n",
    "A **chain** is a sequence of steps (prompts, tools, or other chains) connected into a **single reusable pipeline**.\n",
    "\n",
    "- Think of it as a recipe: each step transforms the input and passes it to the next.\n",
    "- Chains can be simple (prompt -> LLM) or complex (multi-step reasoning + tools).\n",
    "- Agents *use* chains internally, but chains are **deterministic**: the steps are predefined.\n",
    "\n",
    "LangChain lets you build **chains explicitly** (deterministic pipelines) or **implicitly** through agents (dynamic pipelines).\n",
    "\n",
    "- **Explicit chain:** You wire together steps (prompt → model → parsing). The flow is fixed and repeatable.\n",
    "- **Agentic (implicit) chain:** The model decides which steps/tools to run at runtime. The flow can vary across calls. \n",
    "\n",
    "> **Note**: The cool thing about agent chains is that instead of just asking an LLM a question, you can give it **tools** to use, and it will decide when and how to use them to answer your question. For example, you don't need to write code that says \"if the user asks about weather, call the weather tool.\" The agent figures this out on its own!\n",
    "\n",
    "Below is an example of an explicit chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: options pricing\n",
      "- Options pricing involves determining the fair value of a financial derivative based on the underlying asset's price and other market factors.  \n",
      "- The Black-Scholes model is one of the most widely used methods for calculating the theoretical price of European call and put options.  \n",
      "- Factors such as volatility, time to expiration, interest rates, and the underlying asset's price significantly influence the option's premium.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Explicit chain: fixed four-step pipeline\n",
    "# Step 1: Prompt template\n",
    "chain_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\", # sets the global rule for output format: title line + exactly three bullets. It’s treated as higher‑priority instructions.\n",
    "        \"Return output with first line 'Title: {topic}', followed by exactly three bullet points.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\", # supplies the task input (the specific topic) and adds a content constraint (bullets must be full sentences).\n",
    "        \"Topic: {topic}. Each bullet must be a complete sentence.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Step 2: Model call\n",
    "chain_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Step 3: Parse to string\n",
    "parser = StrOutputParser() # converts the raw LLM output into a string.\n",
    "\n",
    "# Step 4: Deterministic formatting\n",
    "format_output = RunnableLambda(\n",
    "    lambda s: \"\\n\".join(\n",
    "        [line for line in [\n",
    "            (\"Title: options pricing\" if not s.strip().split(\"\\n\")[0].startswith(\"Title:\") else s.strip().split(\"\\n\")[0]),\n",
    "            *[\n",
    "                (line if line.strip().startswith(\"-\") else f\"- {line.strip()}\")\n",
    "                for line in s.strip().split(\"\\n\")[1:]\n",
    "                if line.strip()\n",
    "            ][:3]\n",
    "        ] if line]\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = chain_prompt | chain_model | parser | format_output\n",
    "result = chain.invoke({\"topic\": \"options pricing\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build an Explicit Chain\n",
    "\n",
    "Create a chain that produces **three bullet points** about a finance topic. Use an explicit prompt + model pipeline, then invoke it.\n",
    "\n",
    "Hint: Use `ChatPromptTemplate`, compose with `|`, and access `response.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"- Risk-neutral pricing involves valuing financial derivatives under a probability measure where all assets grow at the risk-free rate, eliminating risk premiums.  \\n- It simplifies valuation by allowing expected payoffs to be discounted at the risk-free rate, regardless of investors' risk preferences.  \\n- This approach is fundamental in the Black-Scholes model and other modern option pricing frameworks, enabling consistent and arbitrage-free valuations.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 80, 'prompt_tokens': 21, 'total_tokens': 101, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7f8eb7d1f9', 'id': 'chatcmpl-CyLlPubQWPK3eQFYKtgtASiYXNrfX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bc2c6-ea3a-7592-b0c4-42d1dfcd66ce-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 21, 'output_tokens': 80, 'total_tokens': 101, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# EXERCISE: Build an explicit chain\n",
    "# 1. Create a ChatPromptTemplate with a {topic} variable\n",
    "# 2. Initialize a model with temperature=0\n",
    "# 3. Compose the chain with |\n",
    "# 4. Invoke it with topic=\"risk-neutral pricing\"\n",
    "# 5. Print response.content\n",
    "\n",
    "chain_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\", # sets the global rule for output format: title line + exactly three bullets. It’s treated as higher‑priority instructions.\n",
    "        \"Return output with first line 'Title: {topic}', followed by exactly three bullet points.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\", # supplies the task input (the specific topic) and adds a content constraint (bullets must be full sentences).\n",
    "        \"Topic: {topic}. Each bullet must be a complete sentence.\"\n",
    "    )\n",
    "])  # TODO: Fill this in\n",
    "model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)  # TODO: Fill this in\n",
    "chain = prompt|model  # TODO: Fill this in\n",
    "response = chain.invoke({\"topic\": \"risk-neutral pricing\"})  # TODO: Fill this in\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following will introduce chains implicitly as we build tool-driven workflows, then show how agents extend them with decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating an Agent\n",
    "\n",
    "Let's start by creating a simple agent. You can think of an agent as a **chain with decision-making**: it interprets the user input, decides which tools to call (if any), and produces a final response.\n",
    "\n",
    "An agent needs:\n",
    "1. A **model** (the LLM that does the thinking)\n",
    "2. **Tools** (functions the agent can call)\n",
    "3. A **system prompt** (instructions for the agent)\n",
    "\n",
    "Here's a concrete example of how to create and use an agent with a time tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: The current time in UTC is 12:54 PM on January 15, 2026.\n"
     ]
    }
   ],
   "source": [
    "# Example: Create an agent with a time tool\n",
    "def get_current_time(timezone: str = \"UTC\") -> str:\n",
    "    \"\"\"Get the current time in a specified timezone.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"Current time in {timezone}: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Initialize the model\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Create the agent\n",
    "example_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_current_time],\n",
    "    system_prompt=\"You are a helpful time assistant\"\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "example_response = example_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what time is it?\"}]}\n",
    ")\n",
    "\n",
    "# Access the response\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Basic Agent\n",
    "\n",
    "Now it's your turn! Create your first agent following the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: The weather in San Francisco is always sunny!\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a simple tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "# EXERCISE: Create a basic agent with a weather tool\n",
    "# 1. Initialize a chat model\n",
    "# Hint: Use init_chat_model() from langchain.chat_models with model name \"gpt-4.1-nano-2025-04-14\" or \"gpt-4\"\n",
    "model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key) # TODO: Fill this in\n",
    "\n",
    "# 2. Create an agent using create_agent\n",
    "# Hint: Use create_agent() from langchain.agents with model, tools=[get_weather], and system_prompt=\"You are a helpful assistant\"\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful time assistant\"\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 3. Run the agent with a message asking about the weather in San Francisco\n",
    "# Hint: Use agent.invoke() with {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in San Francisco\"}]}\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in San Francisco?\"}]}\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# Print the response\n",
    "print(\"response:\", response['messages'][-1].content)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Custom Tools\n",
    "\n",
    "Tools are functions that agents can call. LangChain makes it easy to convert Python functions into tools using the `@tool` decorator. The `@tool` decorator:\n",
    "- Automatically extracts function name, description, and parameters\n",
    "- Makes the function available to the agent\n",
    "- Handles type validation and conversion\n",
    "\n",
    "> **Important**: The function's docstring becomes part of the agent's prompt! Make it descriptive so the agent knows when to use the tool.\n",
    "\n",
    "Here's a working example using different tools (string manipulation) to demonstrate the @tool decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: The reversed string is \"dlroW olleH\" and it contains 2 words.\n"
     ]
    }
   ],
   "source": [
    "# Example: Create tools for string manipulation (different from the calculator exercise)\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def reverse_string(text: str) -> str:\n",
    "    \"\"\"Reverse a string.\"\"\"\n",
    "    return text[::-1]\n",
    "\n",
    "@tool\n",
    "def uppercase_string(text: str) -> str:\n",
    "    \"\"\"Convert a string to uppercase.\"\"\"\n",
    "    return text.upper()\n",
    "\n",
    "@tool\n",
    "def count_words(text: str) -> int:\n",
    "    \"\"\"Count the number of words in a string.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "# Create an agent with these string manipulation tools\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "string_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[reverse_string, uppercase_string, count_words],\n",
    "    system_prompt=\"You are a helpful text processing assistant\"\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "example_response = string_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Reverse the string 'Hello World' and count its words\"}]}\n",
    ")\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create a LangChain tools\n",
    "\n",
    "Now it's your turn! Create your first LangChain tools following the syntaxis of the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: First, 15 plus 27 equals 42. Then, multiplying that by 3 gives 126.\n",
      "{'messages': [HumanMessage(content='What is 15 plus 27, then multiply that by 3?', additional_kwargs={}, response_metadata={}, id='fa93ba6c-6e85-47ca-9fd0-1ad3ccd1ccb1'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 112, 'total_tokens': 162, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7f8eb7d1f9', 'id': 'chatcmpl-CyNRy2Li3vmifunE6jXLCoCGbsLeE', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bc329-d7e9-7cd2-902a-e5c85e5dafc3-0', tool_calls=[{'name': 'add', 'args': {'a': 15, 'b': 27}, 'id': 'call_i5MfpgGgxXsQBQCRe3UOUXgx', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_6FJtzXch1B3yGHJsUMdsxHCi', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 112, 'output_tokens': 50, 'total_tokens': 162, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='42.0', name='add', id='0413effd-2149-4066-a9ff-656aa50ef098', tool_call_id='call_i5MfpgGgxXsQBQCRe3UOUXgx'), ToolMessage(content='3.0', name='multiply', id='07867350-fa8e-4227-8286-f4a41a94244f', tool_call_id='call_6FJtzXch1B3yGHJsUMdsxHCi'), AIMessage(content='First, 15 plus 27 equals 42. Then, multiplying that by 3 gives 126.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 182, 'total_tokens': 205, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7f8eb7d1f9', 'id': 'chatcmpl-CyNRzzQ9t0EhjtDvttfc3VdkPzvWQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc329-dd0b-7b01-93fe-17ac7e5b91cf-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 182, 'output_tokens': 23, 'total_tokens': 205, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "### Exercise 2: Create Multiple Tools\n",
    "\n",
    "# EXERCISE: Create a calculator agent with multiple tools\n",
    "# 1. Create a tool for addition\n",
    "# Hint: Use @tool decorator from langchain.tools, function should take (a: float, b: float) and return a + b\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a+b  # TODO: Fill this in (define the function with @tool decorator)\n",
    "\n",
    "# 2. Create a tool for multiplication\n",
    "# Hint: Use @tool decorator, function should take (a: float, b: float) and return a * b\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return a*b  # TODO: Fill this in (define the function with @tool decorator)\n",
    "\n",
    "# 3. Create a tool for getting the square root\n",
    "# Hint: Use @tool decorator and import math, function should take (x: float) and return math.sqrt(x)\n",
    "import math\n",
    "# TODO: Fill this in (define the function with @tool decorator)\n",
    "@tool\n",
    "def sqrt(a: float) -> float:\n",
    "    \"\"\"getting the square root of the definite number.\"\"\"\n",
    "    return math.sqrt(a)\n",
    "# 4. Create an agent with all three tools\n",
    "# Hint: Use create_agent() from langchain.agents with model, tools=[add, multiply, sqrt], and system_prompt=\"You are a helpful calculator assistant\"\n",
    "calculator_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[add, multiply, sqrt],\n",
    "    system_prompt=\"You are a helpful text processing assistant\"\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 5. Test your agent\n",
    "# Hint: Use calculator_agent.invoke() with {\"messages\": [{\"role\": \"user\", \"content\": \"What is 15 plus 27, then multiply that by 3?\"}]}\n",
    "response = calculator_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 15 plus 27, then multiply that by 3?\"}]}\n",
    ")  # TODO: Fill this in\n",
    "print(\"response:\", response['messages'][-1].content)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Behavior**: The agent should:\n",
    "1. First call `add(15, 27)` to get 42\n",
    "2. Then call `multiply(42, 3)` to get 126\n",
    "3. Return the final answer\n",
    "\n",
    "This demonstrates that agents can **chain multiple tool calls** to solve complex problems! The agent automatically figures out the sequence of operations needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tools with Runtime Context\n",
    "\n",
    "Sometimes tools need access to runtime information (like user IDs, session data, etc.). LangChain provides `ToolRuntime` for this. `ToolRuntime` allows tools to access:\n",
    "- **Context**: Custom data passed when invoking the agent\n",
    "- **Memory**: Conversation history and state\n",
    "- **Configuration**: Runtime settings\n",
    "\n",
    "> **Note**: The `ToolRuntime` parameter is automatically injected by LangChain. You don't pass it when calling the tool - LangChain handles that for you!\n",
    "\n",
    "Here is an example that uses `ToolRuntime` to build Context-Aware Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: Based on your preferences, I recommend exploring items in blue color. Would you like some specific suggestions or categories to consider?\n"
     ]
    }
   ],
   "source": [
    "# # Example: Context-aware tool for user preferences (different from greeting exercise)\n",
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Define a context schema with user preferences\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "    favorite_color: str\n",
    "\n",
    "# Create a tool that uses ToolRuntime to access user preferences\n",
    "@tool\n",
    "def get_recommendation(runtime: ToolRuntime[UserContext]) -> str:\n",
    "    \"\"\"Get a personalized recommendation based on user preferences.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    favorite_color = runtime.context.favorite_color\n",
    "    return f\"User {user_id} might like items in {favorite_color} color!\"\n",
    "\n",
    "# Create an agent with this tool\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "recommendation_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_recommendation],\n",
    "    system_prompt=\"You are a helpful recommendation assistant\",\n",
    "    context_schema=UserContext\n",
    ")\n",
    "\n",
    "# Invoke with context\n",
    "example_response = recommendation_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What would you recommend for me?\"}]},\n",
    "    context=UserContext(user_id=\"123\", favorite_color=\"blue\")\n",
    ")\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Context-Aware Tools\n",
    "\n",
    "Now create your own personalized greeting tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: Hello, Alice! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Create a personalized greeting tool using runtime context\n",
    "# 1. Define a Context dataclass with a user_name field\n",
    "# Hint: Use @dataclass from dataclasses, create a class Context with user_name: str field\n",
    "# Context = None  # TODO: Fill this in (define the dataclass)\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_name: str\n",
    "\n",
    "# 2. Create a tool that uses ToolRuntime to access context\n",
    "# Hint: Use @tool from langchain.tools, function parameter should be runtime: ToolRuntime[Context], access user_name via runtime.context.user_name\n",
    "# get_personalized_greeting = None  # TODO: Fill this in (define the function with @tool decorator)\n",
    "@tool\n",
    "def get_personalized_greeting(runtime: ToolRuntime[UserContext]) -> str:\n",
    "    \"\"\"Get a personalized greeting based on user name.\"\"\"\n",
    "    user_name = runtime.context.user_name\n",
    "    \n",
    "    return f\"User {user_name} might like cheerful greeting tone!\"\n",
    "# 3. Create an agent with this tool\n",
    "# Hint: Use create_agent() with model, tools=[get_personalized_greeting], system_prompt, and context_schema=Context\n",
    "personalized_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_personalized_greeting],\n",
    "    system_prompt=\"You are a helpful recommendation assistant\",\n",
    "    context_schema=UserContext\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 4. Invoke the agent with context\n",
    "# Hint: Use personalized_agent.invoke() with messages and context=Context(user_name=\"Alice\")\n",
    "response = personalized_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n",
    "    context=UserContext(user_name=\"Alice\")\n",
    ")\n",
    "print(\"response:\", response['messages'][-1].content)  # TODO: Fill this in\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Memory to Agents\n",
    "\n",
    "So far, our agents don't remember previous conversations. Let's add **memory** so agents can maintain context across multiple interactions. A **checkpointer** stores conversation state, which you can think of as the **state of the agent's chain across turns**. LangChain provides:\n",
    "- `InMemorySaver`: For development/testing (lost when program ends)\n",
    "- Database checkpointers: For production (persistent storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: `ToolRuntime` and `InMemorySaver` both relate to “runtime context,” but they operate at different layers.\n",
    ">- `ToolRuntime` is per-tool-call and injected into tool functions; it provides a view of context/memory/config at that moment.\n",
    ">- `InMemorySaver` is storage, passed to the agent as a checkpointer to persist conversation state between invocations (keyed by `thread_id`). It does not get injected into tools.\n",
    ">- `ToolRuntime` doesn’t store anything by itself; `InMemorySaver` doesn’t provide arbitrary runtime context/config—only persistence for state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to add memory to an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: Here's an inspiring quote about success: \"Success is not final, failure is not fatal: it is the courage to continue that counts.\"\n",
      "Second response: I shared the quote: \"Success is not final, failure is not fatal: it is the courage to continue that counts.\"\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Create a checkpointer\n",
    "example_checkpointer = InMemorySaver()\n",
    "\n",
    "# Create a quote tool (different from the fact tool in the exercise)\n",
    "@tool\n",
    "def get_quote(category: str) -> str:\n",
    "    \"\"\"Get an inspirational quote by category.\"\"\"\n",
    "    quotes = {\n",
    "        \"success\": \"Success is not final, failure is not fatal: it is the courage to continue that counts.\",\n",
    "        \"wisdom\": \"The only true wisdom is in knowing you know nothing.\",\n",
    "        \"motivation\": \"The way to get started is to quit talking and begin doing.\"\n",
    "    }\n",
    "    return quotes.get(category.lower(), \"Here's a quote: Keep moving forward!\")\n",
    "\n",
    "# Create an agent with memory\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "quote_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_quote],\n",
    "    system_prompt=\"You are a helpful assistant that shares inspirational quotes\",\n",
    "    checkpointer=example_checkpointer\n",
    ")\n",
    "\n",
    "# Create a config with thread_id\n",
    "example_config = {\"configurable\": {\"thread_id\": \"quote-session-1\"}}\n",
    "\n",
    "# First message\n",
    "example_response1 = quote_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Give me a quote about success\"}]},\n",
    "    config=example_config\n",
    ")\n",
    "\n",
    "# Second message - agent remembers!\n",
    "example_response2 = quote_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What quote did you just share?\"}]},\n",
    "    config=example_config\n",
    ")\n",
    "print(\"First response:\", example_response1['messages'][-1].content)\n",
    "print(\"Second response:\", example_response2['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: The `thread_id` in the config is crucial! It tells the checkpointer which conversation to load. Different `thread_id` values mean different conversations. This allows you to manage multiple concurrent conversations with the same agent.\n",
    "\n",
    "### Exercise 4: Conversational Memory\n",
    "\n",
    "Now create your own agent with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: Here's an interesting fact about Python: it was named after Monty Python's Flying Circus, not the snake!\n",
      "Second response: The fact I just told you is that Python was named after Monty Python's Flying Circus, not the snake.\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Create an agent with conversational memory\n",
    "# 1. Create an InMemorySaver checkpointer\n",
    "# Hint: Import InMemorySaver from langgraph.checkpoint.memory and create an instance\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "checkpointer = InMemorySaver()  # TODO: Fill this in\n",
    "\n",
    "# 2. Create a simple tool that returns a fact\n",
    "@tool\n",
    "def get_fact(topic: str) -> str:\n",
    "    \"\"\"Get an interesting fact about a topic.\"\"\"\n",
    "    facts = {\n",
    "        \"python\": \"Python was named after Monty Python's Flying Circus\",\n",
    "        \"ai\": \"The term 'artificial intelligence' was coined in 1956\",\n",
    "        \"space\": \"A day on Venus is longer than its year\"\n",
    "    }\n",
    "    return facts.get(topic.lower(), f\"I don't know much about {topic}\")\n",
    "\n",
    "# 3. Create an agent with the checkpointer\n",
    "# Hint: Use create_agent() with model, tools=[get_fact], system_prompt, and checkpointer=checkpointer\n",
    "memory_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_fact],\n",
    "    system_prompt=\"You are a helpful assistant that shares facts\",\n",
    "    checkpointer=example_checkpointer\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 4. Create a config with a thread_id (this identifies the conversation)\n",
    "# Hint: Create a dictionary with {\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation-1\"}}  # TODO: Fill this in\n",
    "\n",
    "# 5. Ask the agent: \"Tell me a fact about Python\"\n",
    "# Hint: Use memory_agent.invoke() with messages and config\n",
    "response1 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a fact about Python\"}]},\n",
    "    config=config\n",
    ")  # TODO: Fill this in\n",
    "# print(\"First response:\", response1)\n",
    "\n",
    "# 6. In a follow-up message, ask: \"What was the fact you just told me?\"\n",
    "# Hint: Use memory_agent.invoke() again with the same config - the agent should remember!\n",
    "response2 =memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What was the fact you just told me?\"}]},\n",
    "    config=config\n",
    ")  # TODO: Fill this in\n",
    "print(\"First response:\", response1['messages'][-1].content)\n",
    "print(\"Second response:\", response2['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structured Output\n",
    "\n",
    "Sometimes you want the agent's response in a specific format. LangChain supports **structured output** using dataclasses or `Pydantic` models. Its functional object for this is `ToolStrategy`, which tells the agent to use tools AND return structured output. The agent will still use tools, but format its final response according to your schema. This gives you the best of both worlds - tool usage with predictable output formats.\n",
    "\n",
    "Here's how to create an agent with structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Name: Laptop\n",
      "Price: 999.0\n",
      "Rating: 4.5\n",
      "Features: ['High performance', 'Lightweight design', 'Long battery life']\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define a different response format\n",
    "@dataclass\n",
    "class ProductRecommendation:\n",
    "    \"\"\"Response schema for product recommendations.\"\"\"\n",
    "    product_name: str\n",
    "    price: float\n",
    "    rating: float = 0.0\n",
    "    features: list[str] = field(default_factory=list)\n",
    "\n",
    "# Create a product search tool\n",
    "@tool\n",
    "def search_products(category: str) -> str:\n",
    "    \"\"\"Search for products in a category.\"\"\"\n",
    "    return f\"Found products in {category}: Laptop ($999, 4.5 stars), Tablet ($499, 4.2 stars)\"\n",
    "\n",
    "# Create an agent with structured output\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "product_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[search_products],\n",
    "    system_prompt=\"You are a helpful product recommendation assistant\",\n",
    "    response_format=ToolStrategy(ProductRecommendation)\n",
    ")\n",
    "\n",
    "# Ask a question and get a structured response\n",
    "example_response = product_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Recommend a laptop for me\"}]}\n",
    ")\n",
    "\n",
    "# Access the structured response\n",
    "structured = example_response['structured_response']\n",
    "print(\"Product Name:\", structured.product_name)\n",
    "print(\"Price:\", structured.price)\n",
    "print(\"Rating:\", structured.rating)\n",
    "print(\"Features:\", structured.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Structured Responses\n",
    "\n",
    "Now create your own agent with structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the available information, there is an 80% confidence that the stock price of NAD will rise tomorrow.\n",
      "Confidence: 0.8\n",
      "Sources: ['Google Search']\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Create an agent with structured output\n",
    "# 1. Define a ResponseFormat dataclass\n",
    "# Hint: Use @dataclass from dataclasses, include answer: str, confidence: float = 0.0, and sources: list[str] = field(default_factory=list)\n",
    "from dataclasses import dataclass, field\n",
    "@dataclass\n",
    "# ResponseFormat = None  # TODO: Fill this in (define the dataclass)\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for answer questions.\"\"\"\n",
    "    answer: str\n",
    "    confidence: float = 0.0\n",
    "    sources: list[str] = field(default_factory=list)\n",
    "# 2. Create a simple tool\n",
    "# Hint: Use @tool from langchain.tools, function should take (query: str) and return a string\n",
    "# search_knowledge_base = None  # TODO: Fill this in (define the function with @tool decorator)\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search for products in a category.\"\"\"\n",
    "    return f\"Found products in {query}: From Google Search, the stock price of NAD has 80% probability to rise tomorrow.\"\n",
    "# 3. Create an agent with structured output\n",
    "# Hint: Use create_agent() with model, tools, system_prompt, and response_format=ToolStrategy(ResponseFormat) from langchain.agents.structured_output\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "structured_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[search_knowledge_base],\n",
    "    system_prompt=\"You are a helpful investment assistant, please give structured answers\" \"When you receive search results, you must extract the probability and set it as 'confidence' (0.0-1.0). \"\n",
    "        \"You must also identify the specific sources mentioned (e.g., Google Search) and list them in 'sources'.\",\n",
    "    response_format=ToolStrategy(ResponseFormat)\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "# 4. Ask a question and get a structured response\n",
    "# Hint: Use structured_agent.invoke() with messages\n",
    "response = structured_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What about the performance of NAD stock tomorrow\"}]}\n",
    ")\n",
    "  # TODO: Fill this in\n",
    "\n",
    "# Access the structured response\n",
    "structured = response['structured_response']\n",
    "print(f\"Answer: {structured.answer}\")\n",
    "print(f\"Confidence: {structured.confidence}\")\n",
    "print(f\"Sources: {structured.sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guided Exercise: Daily S&P 500 Decision with Twitter Sentiment Analysis\n",
    "\n",
    "In this exercise, you will build an agent that decides whether to **BUY** or **NOT BUY** units of the S&P 500 (e.g., SPY), computing **local sentiment** for tweets **before the decision day**. We use the following Hugging Face dataset: https://huggingface.co/datasets/StephanAkkerman/stock-market-tweets-data\n",
    "\n",
    "> **Note**: This is a simplified educational example; don’t take it as financial advice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from datasets)\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Using cached numpy-2.4.1-cp311-cp311-macosx_14_0_arm64.whl (5.5 MB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp311-cp311-macosx_12_0_arm64.whl (34.3 MB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./lang-chain/lib/python3.11/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./lang-chain/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./lang-chain/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./lang-chain/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Using cached multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Collecting fsspec[http]<=2025.10.0,>=2023.1.0 (from datasets)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Using cached huggingface_hub-1.3.2-py3-none-any.whl (534 kB)\n",
      "Requirement already satisfied: packaging in ./lang-chain/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lang-chain/lib/python3.11/site-packages (from datasets) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.3-cp311-cp311-macosx_11_0_arm64.whl (494 kB)\n",
      "Requirement already satisfied: anyio in ./lang-chain/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./lang-chain/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./lang-chain/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./lang-chain/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./lang-chain/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./lang-chain/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./lang-chain/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lang-chain/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./lang-chain/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./lang-chain/lib/python3.11/site-packages (from pandas->datasets) (2025.3)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./lang-chain/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.8.0-cp311-cp311-macosx_11_0_arm64.whl (50 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.7.0-cp311-cp311-macosx_11_0_arm64.whl (44 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.4.1-cp311-cp311-macosx_11_0_arm64.whl (47 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.22.0-cp311-cp311-macosx_11_0_arm64.whl (94 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./lang-chain/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Installing collected packages: pytz, shellingham, pyarrow, propcache, numpy, multidict, hf-xet, fsspec, frozenlist, filelock, dill, click, aiohappyeyeballs, yarl, typer-slim, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 click-8.3.1 datasets-4.5.0 dill-0.4.0 filelock-3.20.3 frozenlist-1.8.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-1.3.2 multidict-6.7.0 multiprocess-0.70.18 numpy-2.4.1 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 shellingham-1.5.4 typer-slim-0.21.1 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56cacddc0944b70917d4c697e622593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d44adf8ad64d9ca112388c50539344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stock-market-tweets-data.csv:   0%|          | 0.00/175M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c1cfbc852943bf9f343a861641fa38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/923673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Build a local pandas dataframe (sampled)\n",
    "ds = load_dataset(\"StephanAkkerman/stock-market-tweets-data\", split=\"train\")\n",
    "df = ds.select(range(20000)).to_pandas()\n",
    "\n",
    "# Normalize and parse dates\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\", utc=True)\n",
    "df = df.dropna(subset=[\"created_at\", \"text\"])\n",
    "df[\"created_at_date\"] = df[\"created_at\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sentiment Score: 0.1207\n",
      "- Recommendation: BUY\n",
      "- Brief Reason: The overall tweet sentiment is positive, indicating bullish market sentiment for SPY.\n"
     ]
    }
   ],
   "source": [
    "# Local sentiment scoring\n",
    "positive_words = {\"gain\", \"gains\", \"bull\", \"bullish\", \"up\", \"upgrade\", \"beat\", \"strong\", \"rally\", \"surge\", \"record\"}\n",
    "negative_words = {\"loss\", \"losses\", \"bear\", \"bearish\", \"down\", \"downgrade\", \"miss\", \"weak\", \"selloff\", \"drop\", \"plunge\"}\n",
    "\n",
    "def sentiment_score(text: str) -> int:\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", text.lower())\n",
    "    score = sum(1 for t in tokens if t in positive_words) - sum(1 for t in tokens if t in negative_words)\n",
    "    return score\n",
    "\n",
    "df[\"sentiment_score\"] = df[\"text\"].fillna(\"\").apply(sentiment_score)\n",
    "\n",
    "# Define the sentiment summary tool\n",
    "# Hint: Use @tool from langchain.tools. Compute statistics from df[\"sentiment_score\"] (e.g. average).\n",
    "@tool\n",
    "def get_sentiment_summary() -> str:\n",
    "    \"\"\"Summarize local tweet sentiment across the full dataset.\"\"\"\n",
    "    avg_score = df[\"sentiment_score\"].mean()\n",
    "    count = len(df)\n",
    "    return f\"Analyzed {count} tweets. The average sentiment score is {avg_score:.4f}.\"\n",
    "    ... # TODO: Fill this in\n",
    "\n",
    "# Write the SYSTEM_PROMPT\n",
    "# Hint: Include goals + rules; Summary must include average sentiment score and key themes.\n",
    "SYSTEM_PROMPT = \"\"\"You are a financial analyst. Your goal is to provide a trading recommendation for SPY.\n",
    "\n",
    "Rules:\n",
    "1. Call 'get_sentiment_summary' to get the market sentiment.\n",
    "2. If the average score > 0, recommend 'BUY'.\n",
    "3. If the average score <= 0, recommend 'NOT BUY'.\n",
    "\n",
    "Output format:\n",
    "- Sentiment Score: [value]\n",
    "- Recommendation: [BUY/NOT BUY]\n",
    "- Brief Reason: [one sentence]\"\"\" # TODO: Fill this in\n",
    "\n",
    "# Initialize the model\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Create the agent and run it\n",
    "# Hint: create_agent(model=example_model, tools=[get_sentiment_summary], system_prompt=SYSTEM_PROMPT)\n",
    "decision_agent = create_agent(\n",
    "    model=example_model, \n",
    "    tools=[get_sentiment_summary], \n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")  # TODO: Fill this in\n",
    "\n",
    "response = decision_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Use the overall tweet sentiment to decide BUY or NOT BUY SPY.\"}]})\n",
    "print(response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congratulations \n",
    "You've completed the LangChain tutorial! We covered\n",
    "\n",
    "- How to create agents with LangChain  \n",
    "- How to build custom tools  \n",
    "- How to add memory to agents  \n",
    "- How to use structured output  \n",
    "- How to build a daily decision agent  \n",
    "\n",
    "### Possible next steps to explore\n",
    "   - **LangGraph**: For more complex agent workflows (see the LangGraph notebook!)\n",
    "   - **Retrieval**: Connect agents to vector databases for RAG\n",
    "   - **Multi-agent systems**: Agents that collaborate\n",
    "   - **LangSmith**: Observability and debugging tools\n",
    "\n",
    "### Additional resources\n",
    "   - [LangChain Docs](https://docs.langchain.com)\n",
    "   - [LangChain Quickstart](https://docs.langchain.com/oss/python/langchain/quickstart)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Happy learning!\n",
    "\n",
    "Pedro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
