{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph:  Agent Orchestration\n",
        "\n",
        "#### By Pedro Izquierdo Lehmann\n",
        "\n",
        "Welcome back! This last notebook is the second part of the LangChain notebook. We will explore the **LangGraph** library, which is built on top of LangChain, and is useful for the controlled design of complex workflows with or without loops, encompassing **agents orchestration**.\n",
        "\n",
        "Also, we will not use the openAI API, but we will query locally stored LLMs using Ollama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## (0. Environment Setup)\n",
        "\n",
        "Before starting, set up a Python virtual environment and install required dependencies.\n",
        "\n",
        "#### 1. Create a Virtual Environment\n",
        "\n",
        "```bash\n",
        "python3 -m venv lang-graph\n",
        "```\n",
        "\n",
        "#### 2. Activate the Virtual Environment\n",
        "\n",
        "**On macOS/Linux:**\n",
        "```bash\n",
        "source lang-graph/bin/activate\n",
        "```\n",
        "\n",
        "**On Windows:**\n",
        "```bash\n",
        "lang-graph\\Scripts\\activate\n",
        "```\n",
        "\n",
        "#### 3. Install Required Dependencies\n",
        "\n",
        "```bash\n",
        "pip install langchain langgraph langchain-ollama jupyter ipykernel\n",
        "```\n",
        "\n",
        "Install Ollama (required for local LLMs):\n",
        "\n",
        "```bash\n",
        "# macOS (Homebrew)\n",
        "brew install ollama\n",
        "brew services start ollama\n",
        "\n",
        "# Linux\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Windows: download the installer\n",
        "# https://ollama.com/download\n",
        "```\n",
        "\n",
        "#### 4. Register the Kernel\n",
        "\n",
        "```bash\n",
        "python -m ipykernel install --user --name=lang-graph --display-name \"Python (lang-graph)\"\n",
        "```\n",
        "\n",
        "#### 5. Start Jupyter\n",
        "\n",
        "```bash\n",
        "jupyter notebook\n",
        "```\n",
        "\n",
        "#### 6. Deactivate\n",
        "\n",
        "```bash\n",
        "deactivate\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# python3 -m venv lang-graph\n",
        "# source lang-graph/bin/activate\n",
        "# lang-graph\\Scripts\\activate\n",
        "# pip install langchain langgraph langchain-ollama jupyter ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ollama (required for local LLMs)\n",
        "# macOS (Homebrew): brew install ollama\n",
        "#                   brew services start ollama\n",
        "# Linux: curl -fsSL https://ollama.com/install.sh | sh\n",
        "# Windows: https://ollama.com/download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# python -m ipykernel install --user --name=lang-graph --display-name \"Python (lang-graph)\"\n",
        "# jupyter notebook\n",
        "# deactivate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ollama Setup\n",
        "\n",
        "All model calls in this notebook must run **locally** via Ollama.\n",
        "\n",
        "1. Start the Ollama server (separate terminal if needed)\n",
        "2. Pull a local model (you can change the model name later)\n",
        "3. Verify that Ollama can list your models\n",
        "\n",
        "Run the code cell below in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check that Ollama is installed and running\n",
        "!ollama --version\n",
        "!ollama list\n",
        "\n",
        "# Pull a local model (edit if you want a different one)\n",
        "OLLAMA_MODEL = \"llama3.1\"\n",
        "!ollama pull {OLLAMA_MODEL}\n",
        "\n",
        "# If Ollama is not running, start it in a separate terminal:\n",
        "# !ollama serve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Literal\n",
        "from dataclasses import dataclass, field\n",
        "import operator\n",
        "import math\n",
        "\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage, BaseMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END, add_messages\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "from ollama import chat\n",
        "from ollama import ChatResponse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In 2018, Phillips 66, a leading American multinational energy company, announced its plans to convert its Ponca City, Oklahoma refinery into a renewable diesel production facility. This significant transformation marked a shift away from traditional fossil fuels and towards more sustainable, low-carbon options.\n",
            "\n",
            "Here's an overview of the conversion:\n",
            "\n",
            "**Background:**\n",
            "\n",
            "The Ponca City Refinery, built in 1919, was one of Phillips 66's largest refineries, with a capacity to process approximately 130,000 barrels per day. The refinery primarily produced gasoline and diesel fuel from crude oil.\n",
            "\n",
            "**Conversion plans:**\n",
            "\n",
            "In 2018, Phillips 66 announced that it would invest around $360 million to convert the Ponca City Refinery into a renewable diesel facility. This project was expected to increase the refinery's capacity by about 50% while reducing greenhouse gas emissions by approximately 40%.\n",
            "\n",
            "The conversion involved installing new equipment and modifying existing facilities to produce renewable diesel, which is typically derived from waste biomass or vegetable oils. The Ponca City Refinery would be upgraded to process renewable feedstocks such as soybean oil, canola oil, and other low-carbon materials.\n",
            "\n",
            "**Conversion process:**\n",
            "\n",
            "To convert the refinery, Phillips 66 employed a range of technologies, including:\n",
            "\n",
            "1. **Renewable diesel production:** The company installed new hydroprocessing units that utilize Fischer-Tropsch (FT) technology to convert waste biomass or vegetable oils into renewable diesel.\n",
            "2. **Feedstock handling:** New storage and handling facilities were built to accommodate the different types of feedstocks used in renewable diesel production.\n",
            "3. **Water treatment:** Phillips 66 invested in state-of-the-art water treatment systems to minimize wastewater generation and ensure compliance with environmental regulations.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "The conversion of the Ponca City Refinery to a renewable diesel facility offered several benefits:\n",
            "\n",
            "1. **Reduced greenhouse gas emissions:** By producing low-carbon fuels, Phillips 66 aimed to decrease the refinery's carbon footprint by approximately 40%.\n",
            "2. **Increased energy independence:** The use of locally sourced feedstocks helped reduce reliance on imported oil and supported local agriculture.\n",
            "3. **Diversified product portfolio:** The conversion enabled Phillips 66 to expand its product offerings, increasing revenue potential and enhancing market competitiveness.\n",
            "\n",
            "**Status update:**\n",
            "\n",
            "While the initial plans called for a phased approach, with some aspects already completed, it appears that the project's timeline has been adjusted due to changes in global markets and regulatory requirements. Phillips 66 may be exploring alternative approaches or collaborating with other companies to accelerate its transition to renewable diesel production.\n",
            "\n",
            "The conversion of Phillips 66's Ponca City Refinery serves as an example of a major oil company adapting to changing market demands, shifting towards more sustainable energy sources while minimizing environmental impacts.\n"
          ]
        }
      ],
      "source": [
        "response: ChatResponse = chat(model='llama3.1', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': 'Please discuss the conversion of Phillips 66 from oil to renewable diesel.',\n",
        "  },\n",
        "])\n",
        "print(response.message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM (local via Ollama)\n",
        "OLLAMA_MODEL = \"llama3.1\"\n",
        "llm = ChatOllama(model=OLLAMA_MODEL, temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Graphs for Agentic AI\n",
        "\n",
        "LangChain gives you **tools** and **agents**, but the control flow still lives inside the LLM loop; the default ReAct pattern is sufficient. Effectively, the chains of LangChain are directed acyclic graphs (DAGs), where \n",
        "\n",
        "- **Nodes** are explicit steps (LLM calls or Python logic)\n",
        "- **Edges** define how control flows between steps\n",
        "- **State** is a typed object that persists across the graph. \n",
        "\n",
        "> **Note**: The graph state is the central, typed data structure that flows through nodes and is mutated by each node. It’s the single source of truth for the workflow at any step (messages, intermediate results, flags, etc.). Every node reads from it and returns partial updates to it.\n",
        "\n",
        "> **Keep in mind**: The graph state, `ToolRuntime`, and `InMemorySaver` all relate to “runtime context,” but they operate at different layers.\n",
        ">- The LangGraph state is the _actual working state_ your nodes read/write.\n",
        ">- `ToolRuntime` is a _tool-level context injection_ (useful for user/session info), not a state container.\n",
        ">- `InMemorySaver` is _storage_ so state can be resumed later; it does not define state, it just saves it.\n",
        "\n",
        "With LangGraph one can construct explicitly a **graph** workflow that may have **cycles**, allowing for more workflow expressivity. Also, it provides conditional routing along the graph; much more control over the workflow is possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "The following is an example of an agentic workflow graph. Notice how we **separate reasoning from deterministic checks**, which is hard to guarantee with a pure agent loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This graph receives financial macros in text and the returns of an asset, then it summarizes the text and computes the volatility of the asset, and then it provides a one-sentence implication for a long-only equity portfolio.\n",
        "# Graph: summarize macro -> compute volatility -> final brief (linear DAG)\n",
        "macro_headlines = [\n",
        "    \"CPI comes in below expectations; core inflation cools\",\n",
        "    \"Fed signals pause; markets price in two cuts\",\n",
        "    \"Oil spikes on supply concerns\",\n",
        "]\n",
        "\n",
        "returns = [0.002, -0.004, 0.001, 0.003, -0.002, 0.0005, -0.001]\n",
        "\n",
        "class MacroState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    macro_summary: str\n",
        "    realized_vol: float\n",
        "    risk_bucket: str\n",
        "\n",
        "\n",
        "def summarize_macro(state: MacroState):\n",
        "    prompt = (\n",
        "        \"Summarize these headlines into a 2-sentence macro brief for an equity analyst:\\n\"\n",
        "        + \"\\n\".join(f\"- {h}\" for h in macro_headlines)\n",
        "    )\n",
        "    response = llm.invoke([SystemMessage(content=prompt)])\n",
        "    return {\"messages\": [response], \"macro_summary\": response.content}\n",
        "\n",
        "\n",
        "def compute_vol(state: MacroState):\n",
        "    daily_vol = math.sqrt(sum(r * r for r in returns) / len(returns))\n",
        "    annualized = daily_vol * math.sqrt(252)\n",
        "    if annualized < 0.15:\n",
        "        bucket = \"LOW\"\n",
        "    elif annualized < 0.25:\n",
        "        bucket = \"MEDIUM\"\n",
        "    else:\n",
        "        bucket = \"HIGH\"\n",
        "    return {\"realized_vol\": annualized, \"risk_bucket\": bucket}\n",
        "\n",
        "\n",
        "def final_brief(state: MacroState):\n",
        "    content = (\n",
        "        f\"Macro brief: {state['macro_summary']}\\n\"\n",
        "        f\"Risk overlay: realized vol {state['realized_vol']:.2%} -> {state['risk_bucket']} risk.\\n\"\n",
        "        \"Provide a one-sentence implication for a long-only equity portfolio.\"\n",
        "    )\n",
        "    response = llm.invoke([HumanMessage(content=content)])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "builder = StateGraph(MacroState)\n",
        "builder.add_node(\"summarize_macro\", summarize_macro)\n",
        "builder.add_node(\"compute_vol\", compute_vol)\n",
        "builder.add_node(\"final_brief\", final_brief)\n",
        "\n",
        "builder.add_edge(START, \"summarize_macro\")\n",
        "builder.add_edge(\"summarize_macro\", \"compute_vol\")\n",
        "builder.add_edge(\"compute_vol\", \"final_brief\")\n",
        "builder.add_edge(\"final_brief\", END)\n",
        "\n",
        "macro_graph = builder.compile()\n",
        "\n",
        "result = macro_graph.invoke({\"messages\": []})\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to visualize the graph structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Mermaid text diagram (paste into a Mermaid renderer if needed)\n",
        "print(macro_graph.get_graph().draw_mermaid())\n",
        "display(Image(macro_graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Add a Position Sizing Node\n",
        "\n",
        "Example above uses a three-step flow: summarize macro, compute volatility, and final brief. Now add a **position sizing** node that sets a target exposure based on the risk bucket:\n",
        "\n",
        "- LOW -> 1.0\n",
        "- MEDIUM -> 0.6\n",
        "- HIGH -> 0.3\n",
        "\n",
        "Then include the target exposure in the final response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCISE: Add a position sizing node\n",
        "# 1. Create a node `position_size(state)` that returns target_exposure\n",
        "# 2. Insert it between compute_vol and final_brief\n",
        "# 3. Update final_brief to include target_exposure\n",
        "\n",
        "class MacroState2(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    macro_summary: str\n",
        "    realized_vol: float\n",
        "    risk_bucket: str\n",
        "    target_exposure: float\n",
        "\n",
        "\n",
        "# TODO: define position_size\n",
        "# def position_size(state: MacroState2):\n",
        "#     ...\n",
        "\n",
        "\n",
        "# TODO: update final_brief to use target_exposure\n",
        "# def final_brief(state: MacroState2):\n",
        "#     ...\n",
        "\n",
        "\n",
        "# TODO: build and run the graph with the new node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Streaming\n",
        "\n",
        "LangGraph supports streaming, allowing you to see agent responses in real-time as they're generated. This is essential for building responsive user interfaces.\n",
        "\n",
        "Below is a simple example that streams node updates as the graph executes. This is especially useful in finance dashboards where you want to show partial progress as each step completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We will use the same macro_graph as before\n",
        "\n",
        "def print_updates(event: dict):\n",
        "    for node, update in event.items():\n",
        "        if isinstance(update, dict) and update.get(\"messages\"):\n",
        "            print(f\"[{node}] {update['messages'][-1].content}\")\n",
        "        else:\n",
        "            print(f\"[{node}] {update}\")\n",
        "\n",
        "\n",
        "for event in macro_graph.stream({\"messages\": []}, stream_mode=\"updates\"):\n",
        "    print_updates(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Conditional Routing (Risk Gates)\n",
        "\n",
        "LangGraph lets you **route** based on state. This is a major difference from LangChain's agent loop, because the control flow is explicit and testable.\n",
        "\n",
        "We will route to a hedge node only when volatility is high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This graph computes realized volatility and uses it as a risk gate to decide whether to propose a hedge or skip hedging.\n",
        "# Graph: compute vol -> route to hedge or skip (conditional gate)\n",
        "class HedgeState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    realized_vol: float\n",
        "    hedge_action: str\n",
        "\n",
        "\n",
        "def compute_vol_only(state: HedgeState):\n",
        "    daily_vol = math.sqrt(sum(r * r for r in returns) / len(returns))\n",
        "    annualized = daily_vol * math.sqrt(252)\n",
        "    return {\"realized_vol\": annualized}\n",
        "\n",
        "\n",
        "def propose_hedge(state: HedgeState):\n",
        "    action = f\"Buy 1-month put spread; realized vol {state['realized_vol']:.2%}.\"\n",
        "    return {\"hedge_action\": action}\n",
        "\n",
        "\n",
        "def skip_hedge(state: HedgeState):\n",
        "    return {\"hedge_action\": \"No hedge needed.\"}\n",
        "\n",
        "\n",
        "def hedge_router(state: HedgeState) -> Literal[\"propose_hedge\", \"skip_hedge\"]:\n",
        "    return \"propose_hedge\" if state[\"realized_vol\"] > 0.25 else \"skip_hedge\"\n",
        "\n",
        "\n",
        "hedge_builder = StateGraph(HedgeState)\n",
        "hedge_builder.add_node(\"compute_vol\", compute_vol_only)\n",
        "hedge_builder.add_node(\"propose_hedge\", propose_hedge)\n",
        "hedge_builder.add_node(\"skip_hedge\", skip_hedge)\n",
        "\n",
        "hedge_builder.add_edge(START, \"compute_vol\")\n",
        "hedge_builder.add_conditional_edges(\"compute_vol\", hedge_router)\n",
        "hedge_builder.add_edge(\"propose_hedge\", END)\n",
        "hedge_builder.add_edge(\"skip_hedge\", END)\n",
        "\n",
        "hedge_graph = hedge_builder.compile()\n",
        "\n",
        "hedge_result = hedge_graph.invoke({\"messages\": []})\n",
        "print(hedge_result[\"hedge_action\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Add a Second Risk Gate\n",
        "\n",
        "Example above uses a volatility gate. Now you have to build a graph with **drawdown gate** that routes to a reduce exposure step if the max drawdown exceeds 6%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCISE: Add a drawdown gate\n",
        "# 1. Compute max drawdown from returns\n",
        "# 2. Create a node reduce_exposure\n",
        "# 3. Route to reduce_exposure when drawdown > 6%\n",
        "\n",
        "returns_ex2 = [0.01, -0.02, 0.005, -0.03, 0.015, -0.01, 0.004]\n",
        "\n",
        "class GateState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    max_drawdown: float\n",
        "    action: str\n",
        "\n",
        "# TODO: implement max drawdown calculation\n",
        "# def compute_drawdown(state: GateState):\n",
        "#     ...\n",
        "\n",
        "# TODO: implement reduce_exposure\n",
        "# def reduce_exposure(state: GateState):\n",
        "#     ...\n",
        "\n",
        "# TODO: implement keep_exposure\n",
        "# def keep_exposure(state: GateState):\n",
        "#     ...\n",
        "\n",
        "# TODO: implement router and graph\n",
        "\n",
        "# TODO: display the graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Durable Execution with Checkpoints\n",
        "\n",
        "In finance, long-running analyses may need to **pause and resume**. LangGraph lets you **checkpoint** state and continue later using a `thread_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This graph creates a single-step thesis draft and checkpoints it so the state can be resumed across sessions.\n",
        "# Graph: single LLM node with checkpointed state\n",
        "class ResearchState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    ticker: str\n",
        "    thesis: str\n",
        "\n",
        "\n",
        "def draft_thesis(state: ResearchState):\n",
        "    prompt = (\n",
        "        f\"Write a 2-sentence investment thesis for {state['ticker']} as a large-cap stock.\"\n",
        "    )\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    return {\"messages\": [response], \"thesis\": response.content}\n",
        "\n",
        "\n",
        "research_builder = StateGraph(ResearchState)\n",
        "research_builder.add_node(\"draft_thesis\", draft_thesis)\n",
        "research_builder.add_edge(START, \"draft_thesis\")\n",
        "research_builder.add_edge(\"draft_thesis\", END)\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "research_graph = research_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"research-1\"}}\n",
        "\n",
        "# First call\n",
        "out1 = research_graph.invoke({\"messages\": [], \"ticker\": \"AAPL\"}, config=config)\n",
        "print(out1[\"thesis\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve the latest checkpointed state\n",
        "state = research_graph.get_state(config)\n",
        "print(state.values[\"ticker\"], \"->\", state.values[\"thesis\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Persistent Analyst Notes\n",
        "\n",
        "Example above stored a thesis in memory. Now add a second node that **appends** a risk note and verify it persists via the state retrieval call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCISE: Add persistent analyst notes\n",
        "# 1. Extend the state with risk_note: str\n",
        "# 2. Add a node add_risk_note that uses the LLM to write 1 sentence\n",
        "# 3. Connect draft_thesis -> add_risk_note -> END\n",
        "# 4. Use get_state to confirm risk_note is stored\n",
        "\n",
        "# TODO: implement here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Agent Nodes with Tools\n",
        "\n",
        "A LangGraph node can wrap a **LangChain agent**. This means you can keep tool-choosing autonomy **inside a node**, while still controlling the workflow **between nodes**.\n",
        "\n",
        "Below we build a small cycle: an agent drafts a trade idea using two tools, then a validator checks the position size. If the size is too large, the graph loops back to the agent for revision.\n",
        "\n",
        "> **Note**: This cycle can't be executed purely with the LangChain framework!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This graph wraps an agent that drafts a trade with tools, validates position size, and loops with a max-cycle limit to avoid infinite revisions.\n",
        "# Graph: agent drafts trade -> validate -> increment cycle -> loop with max cycles\n",
        "import re\n",
        "\n",
        "@tool\n",
        "def get_price(ticker: str) -> float:\n",
        "    \"\"\"Get a rough price for a ticker.\"\"\"\n",
        "    prices = {\"SPY\": 540.0, \"TLT\": 92.0, \"GLD\": 215.0}\n",
        "    return prices.get(ticker.upper(), 100.0)\n",
        "\n",
        "@tool\n",
        "def position_size_from_vol(vol: float, risk_budget: float = 0.01) -> float:\n",
        "    \"\"\"Simple position sizing from volatility and risk budget.\"\"\"\n",
        "    return min(0.15, risk_budget / max(vol, 1e-6))\n",
        "\n",
        "@tool\n",
        "def get_max_cycles(max_cycles: int = 3) -> int:\n",
        "    \"\"\"Return the maximum number of graph cycles allowed.\"\"\"\n",
        "    return max_cycles\n",
        "\n",
        "\n",
        "class AgentNodeState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    draft: str\n",
        "    approved: bool\n",
        "    max_position: float\n",
        "    max_cycles: int\n",
        "    cycle_count: int\n",
        "    stop_reason: str\n",
        "\n",
        "\n",
        "def set_max_cycles(state: AgentNodeState):\n",
        "    requested = state.get(\"max_cycles\", 3)\n",
        "    return {\n",
        "        \"max_cycles\": get_max_cycles.invoke({\"max_cycles\": requested}),\n",
        "        \"cycle_count\": 0,\n",
        "        \"stop_reason\": \"\",\n",
        "    }\n",
        "\n",
        "\n",
        "def agent_trade(state: AgentNodeState):\n",
        "    agent = create_agent(\n",
        "        model=llm,\n",
        "        tools=[get_price, position_size_from_vol],\n",
        "        system_prompt=(\n",
        "            \"You are a trading assistant. Use tools to get prices and position size. \"\n",
        "            \"Always produce a trade with an explicit percent of NAV.\"\n",
        "        ),\n",
        "    )\n",
        "    prompt = (\n",
        "        f\"Draft one trade idea for SPY or TLT. Max position is {state['max_position']:.0%} of NAV. \"\n",
        "        \"Call both tools, then give a 2-3 sentence recommendation with a percent of NAV.\"\n",
        "    )\n",
        "    response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": prompt}]})\n",
        "    message = response[\"messages\"][-1]\n",
        "    return {\"messages\": [message], \"draft\": message.content}\n",
        "\n",
        "\n",
        "def validate_draft(state: AgentNodeState):\n",
        "    percents = [float(x) for x in re.findall(r\"(\\d+(?:\\.\\d+)?)%\", state[\"draft\"])]\n",
        "    if not percents:\n",
        "        approved = False\n",
        "    else:\n",
        "        approved = max(percents) <= state[\"max_position\"] * 100\n",
        "    return {\"approved\": approved}\n",
        "\n",
        "\n",
        "def increment_cycle(state: AgentNodeState):\n",
        "    count = state.get(\"cycle_count\", 0) + 1\n",
        "    stop_reason = \"\"\n",
        "    if count >= state[\"max_cycles\"]:\n",
        "        stop_reason = f\"Reached max cycles ({state['max_cycles']}).\"\n",
        "    return {\"cycle_count\": count, \"stop_reason\": stop_reason}\n",
        "\n",
        "\n",
        "def revision_router(state: AgentNodeState) -> Literal[\"agent_trade\", END]:\n",
        "    if state[\"approved\"]:\n",
        "        return END\n",
        "    if state[\"cycle_count\"] >= state[\"max_cycles\"]:\n",
        "        return END\n",
        "    return \"agent_trade\"\n",
        "\n",
        "\n",
        "def print_state(state: AgentNodeState):\n",
        "    print(\n",
        "        f\"cycle={state.get('cycle_count', 0)}/{state.get('max_cycles', '?')} \"\n",
        "        f\"approved={state.get('approved')} stop_reason={state.get('stop_reason', '')}\"\n",
        "    )\n",
        "    if state.get(\"draft\"):\n",
        "        print(f\"draft: {state['draft']}\")\n",
        "\n",
        "\n",
        "agent_builder = StateGraph(AgentNodeState)\n",
        "agent_builder.add_node(\"set_max_cycles\", set_max_cycles)\n",
        "agent_builder.add_node(\"agent_trade\", agent_trade)\n",
        "agent_builder.add_node(\"validate_draft\", validate_draft)\n",
        "agent_builder.add_node(\"increment_cycle\", increment_cycle)\n",
        "\n",
        "agent_builder.add_edge(START, \"set_max_cycles\")\n",
        "agent_builder.add_edge(\"set_max_cycles\", \"agent_trade\")\n",
        "agent_builder.add_edge(\"agent_trade\", \"validate_draft\")\n",
        "agent_builder.add_edge(\"validate_draft\", \"increment_cycle\")\n",
        "agent_builder.add_conditional_edges(\"increment_cycle\", revision_router)\n",
        "\n",
        "agent_graph = agent_builder.compile()\n",
        "\n",
        "last_state = None\n",
        "for state in agent_graph.stream({\"messages\": [], \"max_position\": 0.1}, stream_mode=\"values\"):\n",
        "    last_state = state\n",
        "    print_state(state)\n",
        "\n",
        "if last_state and last_state.get(\"stop_reason\"):\n",
        "    print(\"Stop reason:\", last_state[\"stop_reason\"])\n",
        "\n",
        "if last_state and last_state.get(\"draft\"):\n",
        "    print(\"Final draft:\", last_state[\"draft\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Portfolio Rebalancing Agent\n",
        "\n",
        "We will build a **multi-step agent** that:\n",
        "\n",
        "1. Ingests market inputs (returns, volatility, rates)\n",
        "2. Classifies the regime (LLM)\n",
        "3. Proposes trades (agent node)\n",
        "4. Runs risk/compliance checks (Python)\n",
        "5. Revises if checks fail (loop)\n",
        "6. Produces a final execution memo\n",
        "\n",
        "This showcases what LangGraph is great at: **explicit looping, validation gates, and stateful orchestration**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This graph drafts a trade, validates it against position limits, and revises until the proposal passes.\n",
        "# Graph: draft -> validate -> revise loop for position limits\n",
        "class LoopState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    proposal: str\n",
        "    approved: bool\n",
        "\n",
        "\n",
        "def draft_trade(state: LoopState):\n",
        "    prompt = \"Propose a single trade idea for SPY with a position size in percent of NAV.\"\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    return {\"messages\": [response], \"proposal\": response.content}\n",
        "\n",
        "\n",
        "def validate_trade(state: LoopState):\n",
        "    # Simple rule: any proposal containing '20%' is rejected\n",
        "    approved = \"20%\" not in state[\"proposal\"]\n",
        "    return {\"approved\": approved}\n",
        "\n",
        "\n",
        "def revise_trade(state: LoopState):\n",
        "    prompt = (\n",
        "        \"Revise the proposal so position size <= 10% of NAV. \"\n",
        "        f\"Original proposal: {state['proposal']}\"\n",
        "    )\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    return {\"messages\": [response], \"proposal\": response.content}\n",
        "\n",
        "\n",
        "def loop_router(state: LoopState) -> Literal[\"revise_trade\", END]:\n",
        "    return \"revise_trade\" if not state[\"approved\"] else END\n",
        "\n",
        "\n",
        "loop_builder = StateGraph(LoopState)\n",
        "loop_builder.add_node(\"draft_trade\", draft_trade)\n",
        "loop_builder.add_node(\"validate_trade\", validate_trade)\n",
        "loop_builder.add_node(\"revise_trade\", revise_trade)\n",
        "\n",
        "loop_builder.add_edge(START, \"draft_trade\")\n",
        "loop_builder.add_edge(\"draft_trade\", \"validate_trade\")\n",
        "loop_builder.add_conditional_edges(\"validate_trade\", loop_router)\n",
        "loop_builder.add_edge(\"revise_trade\", \"validate_trade\")\n",
        "\n",
        "loop_graph = loop_builder.compile()\n",
        "\n",
        "loop_out = loop_graph.invoke({\"messages\": []})\n",
        "print(loop_out[\"proposal\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: Build the Portfolio Rebalancing Graph\n",
        "\n",
        "Use the pattern above to build a full graph. Fill in the TODOs. The end result should produce a final execution memo.\n",
        "\n",
        "You can keep the logic simple—what matters is the **graph structure**, **state**, and **risk gates**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "market_inputs = {\n",
        "    \"returns_1m\": -0.03,\n",
        "    \"vol_1m\": 0.28,\n",
        "    \"rates_change_bps\": -15,\n",
        "    \"sector_momentum\": {\"Tech\": 0.04, \"Financials\": -0.02, \"Energy\": 0.01},\n",
        "}\n",
        "\n",
        "# Make inputs available to the agent tools\n",
        "capstone_inputs = market_inputs\n",
        "\n",
        "class RebalanceState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    inputs: dict\n",
        "    regime: str\n",
        "    trades: list[dict]\n",
        "    checks_passed: bool\n",
        "    memo: str\n",
        "\n",
        "\n",
        "def ingest_inputs(state: RebalanceState):\n",
        "    return {\"inputs\": market_inputs}\n",
        "\n",
        "\n",
        "def risk_checks(state: RebalanceState):\n",
        "    # Simple deterministic checks: gross exposure and sector concentration\n",
        "    gross = sum(abs(t.get(\"weight\", 0.0)) for t in state[\"trades\"])\n",
        "    sector_counts = {}\n",
        "    for t in state[\"trades\"]:\n",
        "        sector_counts[t.get(\"sector\", \"Unknown\")] = sector_counts.get(t.get(\"sector\", \"Unknown\"), 0) + 1\n",
        "    max_sector = max(sector_counts.values()) if sector_counts else 0\n",
        "    passed = gross <= 1.0 and max_sector <= 2\n",
        "    return {\"checks_passed\": passed}\n",
        "\n",
        "\n",
        "# TODO: classify_regime(state) -> set state[\"regime\"]\n",
        "# Hint: use llm.invoke with a prompt that maps inputs to a regime like\n",
        "# \"risk_on\", \"neutral\", \"risk_off\"\n",
        "\n",
        "# Tools for the capstone agent node\n",
        "capstone_inputs = {}\n",
        "\n",
        "@tool\n",
        "def get_sector_momentum(sector: str) -> float:\n",
        "    \"\"\"Return recent momentum for a sector.\"\"\"\n",
        "    return capstone_inputs.get(\"sector_momentum\", {}).get(sector, 0.0)\n",
        "\n",
        "@tool\n",
        "def risk_adjusted_weight(vol: float, base: float = 0.1) -> float:\n",
        "    \"\"\"Suggest a weight based on volatility and a base risk budget.\"\"\"\n",
        "    return min(0.2, max(0.02, base * (0.2 / max(vol, 1e-6))))\n",
        "    \n",
        "# TODO: propose_trades(state) -> set state[\"trades\"]\n",
        "# Hint: ask the LLM to return STRICT JSON list of trades like\n",
        "# [{\"ticker\":\"SPY\",\"weight\":-0.1,\"sector\":\"Index\",\"rationale\":\"...\"}, ...]\n",
        "# Then parse with json.loads\n",
        "\n",
        "# TODO: revise_trades(state) -> adjust trades to pass checks\n",
        "# Hint: tell the LLM to reduce gross exposure and diversify sectors\n",
        "\n",
        "# TODO: final_memo(state) -> create a 5-7 sentence execution memo\n",
        "# Include regime, key inputs, trade list, and risk summary\n",
        "\n",
        "# TODO: router function that loops back to revise_trades if checks fail\n",
        "\n",
        "# TODO: build and run the graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Congratulations!\n",
        "\n",
        "You've completed the LangGraph tutorial! We covered:\n",
        "\n",
        "- Explicit control flow as a graph\n",
        "- Deterministic risk gates alongside LLM reasoning\n",
        "- Durable state and resumable execution\n",
        "- Validation loops for compliance-aware agents\n",
        "\n",
        "### Possible next steps to explore\n",
        "   - **Retrieval**: Connect agents to vector databases for RAG\n",
        "   - **Multi-agent systems**: Agents that collaborate\n",
        "   - **LangSmith**: Observability and debugging tools\n",
        "\n",
        "### Additional resources\n",
        "- [LangGraph Blog](https://www.blog.langchain.com/langgraph/)\n",
        "- [LangGraph Docs](https://docs.langchain.com/oss/python/langgraph/overview)\n",
        "- [Ollama Docs](https://docs.ollama.com/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyTorchFinance",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
